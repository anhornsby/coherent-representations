# -*- coding: utf-8 -*-
# !/usr/bin/env python
# Adam Hornsby & Sebastien Ohleyer

"""
Coherency Maximizing RL agent for sequences of subjective choices
"""

from __future__ import division
import copy
import numpy as np

def categorical_crossentropy(y, y_hat):
    """Computes the categorical crossentropy for a set of predictions p_hat"""

    loss = - np.multiply(y, np.log(y_hat)).sum() / y.shape[0]

    return  loss

def softmax(x, temp=1.0):
    """Softmax function with optional temperature parameter"""

    return np.exp(x / temp) / np.sum(np.exp(x / temp), keepdims=True)

class SoftmaxLayer(object):
    """Softmax layer"""

    def __init__(self, n_input, n_output):
        super(SoftmaxLayer, self).__init__()
        self.n_input = n_input
        self.n_output = n_output

    def feed_forward(self, X):
        self.input_ = X
        self.activations_ = softmax(X)

        return self.activations_

    def loss(self, y, y_hat):
        return categorical_crossentropy(y, y_hat)

    def compute_gradient(self, probs, l_action):
        """
        Returns the gradient of the input with respect to the output
        probs denotes the probabilities generated by the agent and l_action a one-hot vector
        denoting the true action of the agent
        """

        dX = -(l_action - probs)

        return dX

class SimilarityLayer(object):
    """Attention-weighted similarity layer"""

    def __init__(self, n_items, n_attributes, c=1., learn_weights=False, learn_prefs=False, p_init=None, w_init=None):
        super(SimilarityLayer, self).__init__()
        self.c = c
        self.learn_weights = learn_weights
        self.learn_prefs = learn_prefs
        self.n_items = n_items
        self.n_attributes = n_attributes
        self.p_init = p_init
        self.w_init = w_init

        # initialise the preference and attention weights
        self._initialisation()
        self.input = None
        self.distance = None
        self.euclid_distance = None
        self.prefs = None

    def _initialisation(self):
        if self.p_init is None:
            self.preference_ = np.zeros((1, self.n_attributes))
        else:
            self.preference_ = np.array([self.p_init])

        if self.learn_weights:
            if self.w_init is None:
                self.attention_weights_ = np.ones((1, self.n_attributes)) / self.n_attributes
            else:
                self.attention_weights_ = np.array([self.w_init])
        else:
            self.attention_weights_ = np.ones((1, self.n_attributes)).astype(float)

    def feed_forward(self, X):
        """Feed forward the layer"""
        self.input = X

        # calculate euclidean distance
        self.distance = np.subtract(X, self.preference_.reshape(-1,1))
        self.euclid_distance = np.dot(self.attention_weights_, np.square(self.distance))

        # scale the distance by c parameter
        activation = -self.c * self.euclid_distance ** 0.5
        self.activation = activation

        return activation

    def compute_gradient(self, grad):
        """Compute the gradients of the layer's activations"""

        if self.learn_weights:
            # Jacobian matrix W
            grad_distw = (self.c**2/2) * (self.distance.T**2 / self.activation.T)
            # Indeed, grad_w = np.dot(grad, grad_distw) <=> grad_w = np.dot(grad_distw.T, grad.T ).T
            grad_w = np.dot(grad, grad_distw)
        else:
            grad_w = np.zeros((1, self.n_attributes))

        if self.learn_prefs:
            # Jacobian matrix P"""
            grad_distP = -self.c**2 * (self.distance.T * self.attention_weights_) / self.activation.T
            # Indeed, grad_p = np.dot(grad, grad_distP) <=> grad_p = np.dot(grad_distP.T, grad.T ).T
            grad_p = np.dot(grad, grad_distP)
        else:
            grad_p = np.zeros(self.n_attributes)

        return grad_p, grad_w

class CoherencyMaximisingAgent(object):
    """
    Coherency Maximising agent, as described by Hornsby & Love (2019).

    The agent works by maintaining a preference and attention weight vector. It uses
    these to determine the subjective value of a choice. A greedy agent will select actions
    with the highest subjective value.

    Preferences and attention weights are updated through "coherency maximzation". That's to say,
    it updates preferences and attention weights so as to maximize the likelihood of the
    previous choice.

    # Parameters
    n_items (int): How many actions are there to choose from?
    n_attributes (int): How many attributes belong to each choice?
    c (int): The lambda parameter (i.e., the fussiness parameter)
    p_eta (float): The learning rate for the preference vector
    w_eta (float): The learning rate for the attention weight vector

    learn_prefs (bool): Whether or not to update preference vector after choice
    learn_weights (bool): Whether or not to update attention weight vector

    p_init (list): List of values to initialise the preference vector with
    w_init (list): List of values to initialise the weight vector with
    """
    def __init__(self, n_items, n_attributes, c=1., p_eta=0.01, w_eta=0.01,
                 learn_weights=False, learn_prefs=False, p_init=None, w_init=None):
        super(CoherencyMaximisingAgent, self).__init__()
        self.n_items = n_items
        self.n_attributes = n_attributes
        self.c = c
        self.p_eta = p_eta
        self.a_eta = w_eta
        self.learn_weights = learn_weights
        self.learn_prefs = learn_prefs

        self.p_init = p_init
        self.w_init = w_init
        self.p_values = []
        self.w_values = []

        self._initialise_model()

        self.actions_taken_ = 0
        self.streak_ = 0
        self.streak_lengths_ = list()

    def _initialise_model(self):
        """Initialise the layers of the model"""

        self.h0 = SimilarityLayer(self.n_items,
                                  self.n_attributes,
                                  c=self.c,
                                  learn_weights=self.learn_weights,
                                  learn_prefs=self.learn_prefs,
                                  p_init=self.p_init,
                                  w_init=self.w_init)

        self.h1 = SoftmaxLayer(self.n_items, self.n_attributes)

        self.p_values.append(copy.copy(self.h0.preference_))
        self.w_values.append(copy.copy(self.h0.attention_weights_))

    def feed_forward(self, X):
        """feedforward the model using the attributes seen in matrix X"""

        # similarity calculations
        activation = self.h0.feed_forward(X)

        # softmax
        probs = self.h1.feed_forward(activation)[0]

        return probs

    def determine_action(self, observation):
        """Select action using softmax action selection"""

        a_prob = self.feed_forward(observation)
        action = np.random.choice(np.arange(self.n_items), p=a_prob)

        self.actions_taken_ += 1
        self.last_chosen_ = action

        return action

    def update_agent(self, observation, last_action):
        """Update the preference and attention weight vectors of the agent"""

        # create one-hot encoded representation of the action
        l_action = np.zeros(self.n_items)
        l_action[last_action] = 1.

        # calculate probability of choice given observation
        probs = self.feed_forward(observation)

        # calculate gradients
        o_grad = self.h1.compute_gradient(probs=probs, l_action=l_action)
        grad_p, grad_w = self.h0.compute_gradient(o_grad)

        self.grad_p_ = grad_p
        self.grad_w_ = grad_w

        # update values using gradient descent
        if self.learn_prefs:
            new_pref = copy.deepcopy(self.h0.preference_) - self.p_eta * grad_p
            self.h0.preference_ = new_pref.clip(min=0, max=1)
            self.p_values.append(copy.copy(self.h0.preference_))

        if self.learn_weights:
            new_weights = copy.deepcopy(self.h0.attention_weights_ ) - self.a_eta * grad_w
            self.h0.attention_weights_ = new_weights.clip(min=0)
            self.w_values.append(copy.copy(self.h0.attention_weights_))
